{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive Bayes | **Assignment**"
      ],
      "metadata": {
        "id": "BHtnKK3z151-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n"
      ],
      "metadata": {
        "id": "lxptjOis1_Jt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm, primarily used for classification tasks, that identifies the optimal hyperplane to separate data points into different classes. It works by finding the widest possible margin between the classes, maximizing the distance between the separating hyperplane and the closest data points (support vectors). This approach helps SVM generalize well and accurately classify new, unseen data.\n",
        "\n",
        "Here's a breakdown of how it works:\n",
        "\n",
        "1. Finding the Optimal Hyperplane:\n",
        "\n",
        "(A). SVM algorithms aim to find the best hyperplane (a line in 2D, a plane in 3D, or a higher-dimensional surface in n-dimensional space) that effectively divides the data into different classes.\n",
        "\n",
        "(B). The hyperplane is chosen to maximize the margin, which is the distance between the hyperplane and the closest data points from each class.\n",
        "\n",
        "2. Support Vectors:\n",
        "\n",
        "(A). Support vectors are the data points that lie closest to the hyperplane and directly influence its position and orientation.\n",
        "\n",
        "(B). They are the critical data points that define the margin and the decision boundary.\n",
        "\n",
        "(C). Only these support vectors are used to define the separating hyperplane, making the model efficient and less susceptible to overfitting.\n",
        "\n",
        "3. Kernel Trick:\n",
        "\n",
        "(A). SVM can handle non-linearly separable data by using the kernel trick.\n",
        "\n",
        "(B). The kernel function maps the data into a higher-dimensional space where it might become linearly separable.\n",
        "\n",
        "(C). Different kernel functions (linear, polynomial, RBF, etc.) are used based on the nature of the data.\n",
        "\n",
        "4. Classification and Regression:\n",
        "\n",
        "(A). SVM can be used for both classification and regression tasks.\n",
        "\n",
        "(B). Classification: Assigning data points to predefined categories.\n",
        "\n",
        "(C). Regression: Predicting continuous values.\n",
        "\n",
        "In essence, SVM learns a decision boundary that maximizes the separation between classes, using support vectors as key data points to define that boundary."
      ],
      "metadata": {
        "id": "qph9yy_p2JAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM."
      ],
      "metadata": {
        "id": "YUwOwN_539CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hard margin and soft margin SVMs are two approaches to building Support Vector Machines for classification, differing primarily in how they handle data that isn't perfectly linearly separable. Hard margin SVMs require a strict separation of data points with no misclassifications, while soft margin SVMs allow for some misclassifications (or margin violations) to better handle datasets with noise or outliers.\n",
        "\n",
        "**Hard Margin SVM**:\n",
        "\n",
        "1. Strict Separation:\n",
        "It seeks a hyperplane that perfectly separates the data into classes, with no data points allowed on the wrong side of the margin or even within it.\n",
        "2. Ideal for Linearly Separable Data:\n",
        "Hard margin SVMs work best when the data can be divided into distinct classes by a straight line (or hyperplane in higher dimensions).\n",
        "3. Sensitive to Outliers:\n",
        "If there are any outliers or points that don't perfectly fit the pattern, the hard margin approach can be overly sensitive and may not generalize well to new, unseen data.\n",
        "\n",
        "**Soft Margin SVM**:\n",
        "\n",
        "1. Allows Misclassifications:\n",
        "Soft margin SVMs introduce \"slack variables\" to allow some data points to be misclassified or fall within the margin.\n",
        "2. Handles Non-Linearly Separable Data:\n",
        "This makes it suitable for datasets that are not perfectly linearly separable, or contain outliers.\n",
        "3. Balances Margin and Errors:\n",
        "The optimization process in soft margin SVMs aims to find a hyperplane that maximizes the margin while minimizing the number of misclassifications (or the sum of slack variables).\n",
        "4. More Robust:\n",
        "By allowing some errors, soft margin SVMs are generally more robust and less prone to overfitting, leading to better generalization performance on new data.\n",
        "\n",
        "In essence: If your data is linearly separable, a hard margin SVM might be a good choice. However, for real-world data, which often contains noise and outliers, the soft margin approach is generally preferred for its increased flexibility and robustness.\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "AHAqw0gF4DrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case."
      ],
      "metadata": {
        "id": "Svxcp13349v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kernel Trick in Support Vector Machines (SVMs) is a method that allows SVMs to find a linear decision boundary in a higher-dimensional feature space without explicitly transforming the data into that higher dimension. This is particularly useful when the data is not linearly separable in its original feature space. The kernel function calculates the dot product (or a similar measure of similarity) between data points in this higher-dimensional space, effectively avoiding the computationally intensive process of explicit mapping.\n",
        "\n",
        "Example of a Kernel:\n",
        "\n",
        "One common example is the Radial Basis Function (RBF) kernel, also known as the Gaussian kernel. Its formula is:\n",
        "\n",
        "K(x_i, x_j) = exp(-gamma * ||x_i - x_j||^2)\n",
        "\n",
        "where:      \n",
        "1. x_i and x_j are two data points.\n",
        "2. ||x_i-x_j||^2 is the squared Euclidean distance between the two data points.\n",
        "3. gamma is a hyperparameter that controls the influence of individual training samples.\n",
        "\n",
        "**Use Case**:\n",
        "\n",
        "The RBF kernel is widely used for non-linear classification problems where the decision boundary is complex and cannot be represented by a simple straight line or plane.\n",
        "\n",
        "For instance, consider a scenario where data points of two classes form concentric circles in a 2D plane.\n",
        "\n",
        "A linear SVM would fail to separate these classes. However, by using the RBF kernel, the SVM can implicitly map these 2D points into a higher-dimensional space where they become linearly separable, allowing the SVM to find a non-linear decision boundary in the original 2D space (which would appear as a circular boundary in this example).\n",
        "\n",
        "This makes the RBF kernel suitable for image recognition, text classification, and other tasks involving intricate data distributions."
      ],
      "metadata": {
        "id": "MoU9i5fV5IEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?"
      ],
      "metadata": {
        "id": "VgHd5K6G7Kdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Naïve Bayes classifier is a simple probabilistic classifier that applies Bayes' theorem with a strong assumption of independence between features. It's called \"naïve\" because it assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature, given the class label. This \"naivety\" simplifies the calculations but can lead to surprisingly good results, especially in text classification.\n",
        "\n",
        "Elaboration:\n",
        "\n",
        "1. Bayes' Theorem:\n",
        "\n",
        "Naïve Bayes classifiers are based on Bayes' Theorem, which is used to calculate the probability of a hypothesis (e.g., a class label) given some observed evidence (e.g., feature values).\n",
        "\n",
        "2. Conditional Independence Assumption:\n",
        "\n",
        "The \"naïve\" part comes from the assumption that the features used to predict the outcome are conditionally independent of each other, given the class label. In other words, the algorithm assumes that each feature contributes to the probability of the class independently, without considering any dependencies between the features.\n",
        "\n",
        "3. Example:\n",
        "Imagine classifying fruits based on color, shape, and taste. A Naïve Bayes classifier would assume that the color of a fruit is independent of its shape or taste when determining the fruit's type (e.g., apple, orange).\n",
        "\n",
        "**Why \"Naive\"**?\n",
        "\n",
        "This independence assumption is often unrealistic in real-world scenarios, as features are often correlated. For instance, the color and shape of a fruit are often related. However, despite this simplification, Naïve Bayes classifiers are known to perform well in many situations, especially in text classification tasks like spam detection.\n",
        "\n",
        "**Benefits**:\n",
        "\n",
        "Despite the \"naïve\" assumption, Naïve Bayes classifiers are easy to implement, computationally efficient, and can be effective even with limited training data."
      ],
      "metadata": {
        "id": "20Z6axam7PGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?"
      ],
      "metadata": {
        "id": "D8IpDr8R8g3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes classifiers come in several variants, each suited for different types of data. Gaussian Naive Bayes is best for continuous, normally distributed data. Multinomial Naive Bayes excels with discrete data, particularly word counts in text. Bernoulli Naive Bayes is appropriate for binary or boolean data, such as word presence/absence in text.\n",
        "\n",
        "**Gaussian Naive Bayes**:\n",
        "\n",
        "1. Data Type: Continuous numerical data (e.g., sensor readings, measurements).\n",
        "2. Assumption: Features follow a normal (Gaussian) distribution within each class.\n",
        "3. Use Cases:\n",
        "\n",
        "(A). Predicting house prices based on features like area and number of bedrooms.\n",
        "\n",
        "(B). Classifying medical diagnoses based on continuous physiological measurements.\n",
        "\n",
        "(C). Analyzing sensor data for anomaly detection.\n",
        "4. Example: If you're classifying flower species based on petal length, you'd use Gaussian Naive Bayes if petal length tends to follow a normal distribution within each species.\n",
        "\n",
        "**Multinomial Naive Bayes**:\n",
        "\n",
        "1. Data Type: Discrete, count-based data (e.g., word frequencies in text).\n",
        "2. Assumption: Features represent the number of occurrences of discrete events.\n",
        "3. Use Cases:\n",
        "\n",
        "(A). Text classification (spam detection, sentiment analysis).\n",
        "\n",
        "(B). Document categorization.\n",
        "\n",
        "(C). Recommender systems based on item counts.\n",
        "4. Example: In spam detection, the algorithm would analyze how many times certain words appear in an email to determine if it's spam.\n",
        "\n",
        "**Bernoulli Naive Bayes**:\n",
        "\n",
        "1. Data Type:\n",
        "Binary or Boolean data (e.g., presence or absence of a word in a document).\n",
        "2. Assumption:\n",
        "Features represent the presence or absence of an event.\n",
        "3. Use Cases:\n",
        "\n",
        "(A). Text classification where you only care about whether a word is present or not (e.g., bag-of-words model).\n",
        "\n",
        "(B). Document classification where you focus on feature presence/absence.\n",
        "4. Example:\n",
        "In text classification, it would determine if a word is present in a document or not, regardless of its frequency.\n",
        "\n",
        "Choosing the Right Variant:\n",
        "\n",
        "1. Data type:\n",
        "Carefully examine your data and choose the variant that best matches its characteristics.\n",
        "2. Experimentation:\n",
        "If unsure, try different variants and compare their performance on a validation set.\n",
        "3. Scikit-learn:\n",
        "The Scikit-learn library provides implementations of all three Naive Bayes variants."
      ],
      "metadata": {
        "id": "VWT9uwVz8pk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "GS4xSv9S_eS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train an SVM Classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 5. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 6. Print the results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"\\nSupport Vectors:\\n\", svm_model.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pIW7Y27IQbt",
        "outputId": "c48d0085-c9ac-4fd0-f4e3-0abe5e7e5526"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "wq2RIQuyInUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 5. Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK9e1gP8Irw-",
        "outputId": "82fffb33-fc79-4d0e-8049-99c4fdd23951"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "a7Kds7U-I6dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# 4. Create the SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "# 5. Apply GridSearchCV\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Best model from GridSearch\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 7. Predict and calculate accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 8. Print results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy1mrLmRJCEa",
        "outputId": "516a0dda-6995-4387-e24a-51e3d579497e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "tyjxoI3JJN6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['rec.sport.baseball', 'sci.space']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = data.data\n",
        "y = data.target  # 0 or 1\n",
        "\n",
        "# 2. Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Naïve Bayes Classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict probabilities for ROC-AUC\n",
        "y_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 6. Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# 7. Print result\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPydjf3FJS7M",
        "outputId": "55b15d4f-3321-4d24-cf3e-d45cae673fe8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9962543953523926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "EBosqRSHJlAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach\n",
        "1. Preprocessing\n",
        "\n",
        "Missing Data:\n",
        "\n",
        "Remove rows with completely empty email content.\n",
        "\n",
        "Replace missing parts (e.g., subject) with a placeholder like \"missing_text\".\n",
        "\n",
        "Text Cleaning:\n",
        "\n",
        "Lowercase text, remove HTML tags, URLs, and non-alphanumeric characters.\n",
        "\n",
        "Vectorization:\n",
        "\n",
        "Use TF-IDF (TfidfVectorizer) to convert text into numerical features, possibly with n-grams (bigrams, trigrams) for spam-specific phrases.\n",
        "\n",
        "2. Model Choice\n",
        "\n",
        "Baseline: Multinomial Naïve Bayes — works very well with sparse TF-IDF features and is computationally efficient.\n",
        "\n",
        "Alternative: Linear SVM (SVC(kernel='linear') or LinearSVC) — may achieve slightly better accuracy but is slower to train.\n",
        "\n",
        "For demonstration, I’ll use Naïve Bayes here.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "Use class_weight='balanced' (if using SVM) or adjust probability threshold for Naïve Bayes.\n",
        "\n",
        "Optionally, apply oversampling (e.g., SMOTE) or undersampling.\n",
        "\n",
        "Focus on metrics beyond accuracy.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Precision: Avoid false positives (important for not flagging legit emails as spam).\n",
        "\n",
        "Recall: Catch as many spam emails as possible.\n",
        "\n",
        "F1-Score: Balance between precision and recall.\n",
        "\n",
        "ROC-AUC & PR-AUC: Robust for imbalanced datasets.\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Security: Blocks phishing and malware-laden spam.\n",
        "\n",
        "Productivity: Saves employees’ time from manual filtering.\n",
        "\n",
        "Customer Trust: Ensures legitimate communication is not lost.\n",
        "\n",
        "Program of **Python**"
      ],
      "metadata": {
        "id": "APimRH93ONOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load a binary dataset (simulate spam vs ham)\n",
        "categories = ['rec.sport.baseball', 'sci.space']  # 'baseball' = ham, 'space' = spam (just for demo)\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = data.data\n",
        "y = data.target  # 0 = baseball (ham), 1 = space (spam)\n",
        "\n",
        "# 2. Handle missing data: replace empty emails with placeholder\n",
        "X = [\"missing_text\" if text.strip() == \"\" else text for text in X]\n",
        "\n",
        "# 3. Vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 4. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 5. Train Naïve Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "y_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 7. Evaluation\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=categories))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQYfE7u1OHD2",
        "outputId": "e57aadc8-3f94-4ecc-8881-2075779bc958"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "rec.sport.baseball       0.95      0.98      0.97       199\n",
            "         sci.space       0.98      0.94      0.96       198\n",
            "\n",
            "          accuracy                           0.96       397\n",
            "         macro avg       0.97      0.96      0.96       397\n",
            "      weighted avg       0.97      0.96      0.96       397\n",
            "\n",
            "ROC-AUC Score: 0.9953048068625958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BS_E-BxGPUjf"
      }
    }
  ]
}